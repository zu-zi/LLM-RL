# bash setup_sglang.sh                # é»˜è®¤ï¼šenv=sgl-cu124ï¼Œpython=3.10ï¼Œä½¿ç”¨æ¸…åé•œåƒåŠ é€Ÿï¼Œå®‰è£…cuda-toolkit
# bash setup_sglang.sh --env-name myenv --py 3.11 --no-mirror --no-cuda-toolkit

#!/usr/bin/env bash
# setup_sglang.sh
# ä¸€é”®æ­å»º SGLang ç¯å¢ƒï¼ˆPyTorch 2.5.1 + CUDA 12.4ï¼‰å¹¶è‡ªæµ‹

set -euo pipefail

########## å¯è°ƒå‚æ•°ï¼ˆä¹Ÿå¯ç”¨å‘½ä»¤è¡Œè¦†ç›–ï¼‰ ##########
ENV_NAME="sgl-cu124"
PY_VER="3.10"
USE_MIRROR=1           # 1=ä½¿ç”¨æ¸…åé•œåƒåŠ é€Ÿï¼ˆä»…é’ˆå¯¹é torch/flashinfer åŒ…ï¼‰, 0=ä¸ç”¨
INSTALL_CUDA_TOOLKIT=1 # 1=å®‰è£…cuda-toolkit=12.4ï¼ˆæä¾›nvccï¼‰ï¼Œ0=ä¸è£…

# ç‰ˆæœ¬å›ºå®šï¼ˆç¨³å®šç»„åˆï¼‰
TORCH_VER="2.5.1"
TV_VER="0.20.1"
TA_VER="2.5.1"

########## è§£æå‘½ä»¤è¡Œ ##########
while [[ $# -gt 0 ]]; do
  case "$1" in
    --env-name) ENV_NAME="$2"; shift 2;;
    --py) PY_VER="$2"; shift 2;;
    --no-mirror) USE_MIRROR=0; shift 1;;
    --no-cuda-toolkit) INSTALL_CUDA_TOOLKIT=0; shift 1;;
    -h|--help)
      echo "Usage: bash setup_sglang.sh [--env-name NAME] [--py 3.10|3.11] [--no-mirror] [--no-cuda-toolkit]"
      exit 0;;
    *) echo "Unknown arg: $1" && exit 1;;
  esac
done

########## å‡½æ•° ##########
die() { echo "ERROR: $*" >&2; exit 1; }

need_cmd() {
  command -v "$1" >/dev/null 2>&1 || die "ç¼ºå°‘å‘½ä»¤ï¼š$1"
}

########## é¢„æ£€æŸ¥ ##########
need_cmd bash
need_cmd python || true
need_cmd conda

# è®© conda å¯è¢«éäº¤äº’è„šæœ¬æ¿€æ´»
CONDA_BASE="$(conda info --base 2>/dev/null)"
source "$CONDA_BASE/etc/profile.d/conda.sh"

echo "=== é…ç½®å‚æ•° ==="
echo "ENV_NAME           = $ENV_NAME"
echo "PY_VER             = $PY_VER"
echo "USE_MIRROR         = $USE_MIRROR"
echo "INSTALL_CUDA_TOOLKIT = $INSTALL_CUDA_TOOLKIT"
echo "TORCH/TV/TA        = $TORCH_VER / $TV_VER / $TA_VER"
echo

########## åˆ›å»º/æ¿€æ´»ç¯å¢ƒ ##########
if conda env list | awk '{print $1}' | grep -qx "$ENV_NAME"; then
  echo "[1/7] å·²å­˜åœ¨ç¯å¢ƒ $ENV_NAME"
else
  echo "[1/7] åˆ›å»º conda ç¯å¢ƒï¼š$ENV_NAME (python=$PY_VER)"
  conda create -n "$ENV_NAME" "python=$PY_VER" -y
fi
conda activate "$ENV_NAME"

########## å¯é€‰ï¼šé•œåƒé…ç½® ##########
if [[ $USE_MIRROR -eq 1 ]]; then
  export PIP_INDEX_URL="https://pypi.tuna.tsinghua.edu.cn/simple"
  echo "[2/7] å·²å¯ç”¨æ¸…å PyPI é•œåƒåŠ é€Ÿï¼ˆä¸å½±å“ torch/flashinfer çš„ä¸“ç”¨ç´¢å¼•ï¼‰"
else
  unset PIP_INDEX_URL || true
  echo "[2/7] æœªå¯ç”¨é•œåƒï¼ˆå…¨éƒ¨èµ°å®˜æ–¹æºï¼‰"
fi

python - <<'PY'
import sys, platform
print("[INFO] Python:", sys.version.split()[0], "| Platform:", platform.platform())
PY

########## å®‰è£… PyTorch 2.5.1 + cu124 ##########
echo "[3/7] å®‰è£… PyTorch/cu124 åŠç›¸å…³ç»„ä»¶ï¼ˆå®˜æ–¹ cu124 æºï¼‰..."
python -m pip install -U pip
pip install "torch==${TORCH_VER}" "torchvision==${TV_VER}" "torchaudio==${TA_VER}" \
  --index-url https://download.pytorch.org/whl/cu124

python - <<'PY'
import torch
print("[CHECK] torch:", torch.__version__, "| cuda runtime from torch:", torch.version.cuda)
print("[CHECK] CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("[CHECK] Device 0:", torch.cuda.get_device_name(0))
PY

########## ï¼ˆå¯é€‰ï¼‰å®‰è£… cuda-toolkit ä»¥æä¾› nvcc ##########
if [[ $INSTALL_CUDA_TOOLKIT -eq 1 ]]; then
  echo "[4/7] å®‰è£… cuda-toolkit=12.4ï¼ˆnvccï¼Œç”¨äºç¼–è¯‘CUDAæ‰©å±•æ›´ç¨³ï¼‰..."
  conda install -y -c nvidia cuda-toolkit=12.4
  export CUDA_HOME="$CONDA_PREFIX"
  export PATH="$CUDA_HOME/bin:$PATH"
  export LD_LIBRARY_PATH="$CUDA_HOME/lib64:${LD_LIBRARY_PATH:-}"
  if command -v nvcc >/dev/null 2>&1; then
    echo "[CHECK] nvcc å­˜åœ¨ï¼š$(nvcc --version | sed -n 's/.*release \([0-9.]*\).*/\1/p' | head -n1)"
  else
    echo "[WARN] æœªæ‰¾åˆ° nvccï¼ˆé€šå¸¸ä¹Ÿå¯ä¸è£…ï¼Œä½†ç¼–è¯‘æ‰©å±•æ—¶å¯èƒ½éœ€è¦ï¼‰"
  fi
else
  echo "[4/7] è·³è¿‡å®‰è£… cuda-toolkitï¼ˆå¦‚åç»­éœ€ç¼–è¯‘æ‰©å±•ï¼Œå¯è‡ªè¡Œå®‰è£…ï¼‰"
fi

########## å®‰è£… sglangã€sgl-kernelã€flashinferã€NLP ä¾èµ– ##########
echo "[5/7] å®‰è£… sglangï¼ˆé•œåƒæˆ–å®˜æ–¹ï¼‰..."
pip install "sglang[all]"

echo "[5/7] å®‰è£… sgl-kernel..."
pip install -U sgl-kernel

echo "[5/7] å®‰è£… flashinferï¼ˆtorch2.5 + cu124 ä¸“ç”¨è½®å­ï¼‰..."
pip install flashinfer \
  --find-links https://flashinfer.ai/whl/cu124/torch2.5/flashinfer/

echo "[5/7] å®‰è£… Transformers/Accelerate/Tiktoken..."
pip install transformers accelerate tiktoken

########## ç‰ˆæœ¬æ‰“å° ##########
python - <<'PY'
import sglang, torch
print("[CHECK] sglang:", sglang.__version__)
print("[CHECK] torch:", torch.__version__, "| torch.cuda:", torch.version.cuda, "| cuda_available:", torch.cuda.is_available())
PY

########## ç”Ÿæˆå¹¶è¿è¡Œ SGLang ç¦»çº¿å¼•æ“æµ‹è¯• ##########
echo "[6/7] ç”Ÿæˆæµ‹è¯•è„šæœ¬ test_sglang_offline.py ..."
cat > test_sglang_offline.py <<'PY'
import torch
import sglang as sgl
from transformers import AutoTokenizer

def test_basic():
    print("="*60)
    print("åŸºæœ¬åŠŸèƒ½è‡ªæ£€")
    print("torch:", torch.__version__, "cuda:", torch.version.cuda)
    print("CUDA available:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("GPU:", torch.cuda.get_device_name(0))
    print("SGLang:", sgl.__version__)

def test_engine():
    print("="*60)
    print("åˆå§‹åŒ– SGLang ç¦»çº¿å¼•æ“ï¼ˆTransformers åç«¯ï¼‰")
    llm = sgl.Engine(
        model_path="gpt2",          # å¯æ¢æˆ "Qwen/Qwen2.5-1.5B-Instruct" ç­‰æ›´å¤§çš„æ¨¡å‹
        impl="transformers",        # ç›´æ¥èµ° HF Transformers åç«¯
        tokenizer="gpt2",
        disable_cuda_graph=True     # å°æ¨¡å‹/å•å¡æ›´ç¨³
    )
    prompt = "Hello, how are you?"
    sampling = {
        "max_new_tokens": 20,
        "temperature": 0.7,
        "stop_token_ids": [AutoTokenizer.from_pretrained("gpt2").eos_token_id],
    }
    print("è¾“å…¥ï¼š", prompt)
    out = llm.generate([prompt], sampling)
    print("è¾“å‡ºï¼š", out[0])
    llm.shutdown()

if __name__ == "__main__":
    test_basic()
    test_engine()
    print("\nğŸ‰ OK! SGLang ç¦»çº¿å¼•æ“è·‘é€šã€‚")
PY

echo "[7/7] è¿è¡Œæµ‹è¯•è„šæœ¬ ..."
python test_sglang_offline.py

echo
echo "============================== DONE =============================="
echo "ç¯å¢ƒï¼š$ENV_NAME  å·²é…ç½®å®Œæ¯•å¹¶é€šè¿‡æµ‹è¯•ã€‚"
echo "å¦‚éœ€å†æ¬¡ä½¿ç”¨ï¼š  conda activate $ENV_NAME"
echo "æµ‹è¯•è„šæœ¬ï¼š       $(pwd)/test_sglang_offline.py"
